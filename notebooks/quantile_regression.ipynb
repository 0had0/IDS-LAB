{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Quantile Regression\n",
        "\n",
        "    .. versionadded:: 2.0.0\n",
        "\n",
        "The script is inspired by this awesome example in sklearn:\n",
        "https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "ename": "XGBoostError",
          "evalue": "[16:58:55] ../src/objective/objective.cc:26: Unknown objective function: `reg:quantileerror`\nObjective candidate: survival:aft\nObjective candidate: binary:hinge\nObjective candidate: multi:softmax\nObjective candidate: multi:softprob\nObjective candidate: rank:pairwise\nObjective candidate: rank:ndcg\nObjective candidate: rank:map\nObjective candidate: reg:squarederror\nObjective candidate: reg:squaredlogerror\nObjective candidate: reg:logistic\nObjective candidate: binary:logistic\nObjective candidate: binary:logitraw\nObjective candidate: reg:linear\nObjective candidate: reg:pseudohubererror\nObjective candidate: count:poisson\nObjective candidate: survival:cox\nObjective candidate: reg:gamma\nObjective candidate: reg:tweedie\nObjective candidate: reg:absoluteerror\n\nStack trace:\n  [bt] (0) /home/hadih/.cache/pypoetry/virtualenvs/fids-rgvgQkLu-py3.11/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x3708b3) [0x7fee83b708b3]\n  [bt] (1) /home/hadih/.cache/pypoetry/virtualenvs/fids-rgvgQkLu-py3.11/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x370f8f) [0x7fee83b70f8f]\n  [bt] (2) /home/hadih/.cache/pypoetry/virtualenvs/fids-rgvgQkLu-py3.11/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x2e543c) [0x7fee83ae543c]\n  [bt] (3) /home/hadih/.cache/pypoetry/virtualenvs/fids-rgvgQkLu-py3.11/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x2ebfd7) [0x7fee83aebfd7]\n  [bt] (4) /home/hadih/.cache/pypoetry/virtualenvs/fids-rgvgQkLu-py3.11/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(XGBoosterBoostedRounds+0x31) [0x7fee8393c081]\n  [bt] (5) /usr/lib/libffi.so.8(+0x74f6) [0x7fef112e54f6]\n  [bt] (6) /usr/lib/libffi.so.8(+0x3f5e) [0x7fef112e1f5e]\n  [bt] (7) /usr/lib/libffi.so.8(ffi_call+0x123) [0x7fef112e4b73]\n  [bt] (8) /usr/lib/python3.11/lib-dynload/_ctypes.cpython-311-x86_64-linux-gnu.so(+0x8f6c) [0x7fef112f7f6c]\n\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 114\u001b[0m\n\u001b[1;32m    103\u001b[0m         plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m    106\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    107\u001b[0m     \u001b[39m# parser = argparse.ArgumentParser()\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[39m# parser.add_argument(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[39m# )\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[39m# args = parser.parse_args()\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     quantile_loss(args\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
            "Cell \u001b[0;32mIn[4], line 41\u001b[0m, in \u001b[0;36mquantile_loss\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m# use Xy as a reference\u001b[39;00m\n\u001b[1;32m     39\u001b[0m Xy_test \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mQuantileDMatrix(X_test, y_test, ref\u001b[39m=\u001b[39mXy)\n\u001b[0;32m---> 41\u001b[0m booster \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m     42\u001b[0m     {\n\u001b[1;32m     43\u001b[0m         \u001b[39m# Use the quantile objective function.\u001b[39;49;00m\n\u001b[1;32m     44\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mobjective\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mreg:quantileerror\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     45\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtree_method\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mhist\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     46\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mquantile_alpha\u001b[39;49m\u001b[39m\"\u001b[39;49m: alpha,\n\u001b[1;32m     47\u001b[0m         \u001b[39m# Let's try not to overfit.\u001b[39;49;00m\n\u001b[1;32m     48\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mlearning_rate\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m0.04\u001b[39;49m,\n\u001b[1;32m     49\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmax_depth\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m5\u001b[39;49m,\n\u001b[1;32m     50\u001b[0m     },\n\u001b[1;32m     51\u001b[0m     Xy,\n\u001b[1;32m     52\u001b[0m     num_boost_round\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m     53\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     54\u001b[0m     \u001b[39m# The evaluation result is a weighted average across multiple quantiles.\u001b[39;49;00m\n\u001b[1;32m     55\u001b[0m     evals\u001b[39m=\u001b[39;49m[(Xy, \u001b[39m\"\u001b[39;49m\u001b[39mTrain\u001b[39;49m\u001b[39m\"\u001b[39;49m), (Xy_test, \u001b[39m\"\u001b[39;49m\u001b[39mTest\u001b[39;49m\u001b[39m\"\u001b[39;49m)],\n\u001b[1;32m     56\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m     57\u001b[0m )\n\u001b[1;32m     58\u001b[0m xx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39matleast_2d(np\u001b[39m.\u001b[39mlinspace(\u001b[39m0\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m1000\u001b[39m))\u001b[39m.\u001b[39mT\n\u001b[1;32m     59\u001b[0m scores \u001b[39m=\u001b[39m booster\u001b[39m.\u001b[39minplace_predict(xx)\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/fids-rgvgQkLu-py3.11/lib/python3.11/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/fids-rgvgQkLu-py3.11/lib/python3.11/site-packages/xgboost/training.py:180\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    168\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    169\u001b[0m         EarlyStopping(rounds\u001b[39m=\u001b[39mearly_stopping_rounds, maximize\u001b[39m=\u001b[39mmaximize)\n\u001b[1;32m    170\u001b[0m     )\n\u001b[1;32m    171\u001b[0m cb_container \u001b[39m=\u001b[39m CallbackContainer(\n\u001b[1;32m    172\u001b[0m     callbacks,\n\u001b[1;32m    173\u001b[0m     metric\u001b[39m=\u001b[39mmetric_fn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     output_margin\u001b[39m=\u001b[39m\u001b[39mcallable\u001b[39m(obj) \u001b[39mor\u001b[39;00m metric_fn \u001b[39mis\u001b[39;00m feval,\n\u001b[1;32m    178\u001b[0m )\n\u001b[0;32m--> 180\u001b[0m bst \u001b[39m=\u001b[39m cb_container\u001b[39m.\u001b[39;49mbefore_training(bst)\n\u001b[1;32m    182\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start_iteration, num_boost_round):\n\u001b[1;32m    183\u001b[0m     \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/fids-rgvgQkLu-py3.11/lib/python3.11/site-packages/xgboost/callback.py:148\u001b[0m, in \u001b[0;36mCallbackContainer.before_training\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''Function called before training.'''\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m--> 148\u001b[0m     model \u001b[39m=\u001b[39m c\u001b[39m.\u001b[39;49mbefore_training(model\u001b[39m=\u001b[39;49mmodel)\n\u001b[1;32m    149\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbefore_training should return the model\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    150\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_cv:\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/fids-rgvgQkLu-py3.11/lib/python3.11/site-packages/xgboost/callback.py:350\u001b[0m, in \u001b[0;36mEarlyStopping.before_training\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbefore_training\u001b[39m(\u001b[39mself\u001b[39m, model: _Model) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m _Model:\n\u001b[0;32m--> 350\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstarting_round \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mnum_boosted_rounds()\n\u001b[1;32m    351\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/fids-rgvgQkLu-py3.11/lib/python3.11/site-packages/xgboost/core.py:2469\u001b[0m, in \u001b[0;36mBooster.num_boosted_rounds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2467\u001b[0m rounds \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m   2468\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2469\u001b[0m _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterBoostedRounds(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, ctypes\u001b[39m.\u001b[39;49mbyref(rounds)))\n\u001b[1;32m   2470\u001b[0m \u001b[39mreturn\u001b[39;00m rounds\u001b[39m.\u001b[39mvalue\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/fids-rgvgQkLu-py3.11/lib/python3.11/site-packages/xgboost/core.py:279\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \n\u001b[1;32m    270\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
            "\u001b[0;31mXGBoostError\u001b[0m: [16:58:55] ../src/objective/objective.cc:26: Unknown objective function: `reg:quantileerror`\nObjective candidate: survival:aft\nObjective candidate: binary:hinge\nObjective candidate: multi:softmax\nObjective candidate: multi:softprob\nObjective candidate: rank:pairwise\nObjective candidate: rank:ndcg\nObjective candidate: rank:map\nObjective candidate: reg:squarederror\nObjective candidate: reg:squaredlogerror\nObjective candidate: reg:logistic\nObjective candidate: binary:logistic\nObjective candidate: binary:logitraw\nObjective candidate: reg:linear\nObjective candidate: reg:pseudohubererror\nObjective candidate: count:poisson\nObjective candidate: survival:cox\nObjective candidate: reg:gamma\nObjective candidate: reg:tweedie\nObjective candidate: reg:absoluteerror\n\nStack trace:\n  [bt] (0) /home/hadih/.cache/pypoetry/virtualenvs/fids-rgvgQkLu-py3.11/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x3708b3) [0x7fee83b708b3]\n  [bt] (1) /home/hadih/.cache/pypoetry/virtualenvs/fids-rgvgQkLu-py3.11/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x370f8f) [0x7fee83b70f8f]\n  [bt] (2) /home/hadih/.cache/pypoetry/virtualenvs/fids-rgvgQkLu-py3.11/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x2e543c) [0x7fee83ae543c]\n  [bt] (3) /home/hadih/.cache/pypoetry/virtualenvs/fids-rgvgQkLu-py3.11/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x2ebfd7) [0x7fee83aebfd7]\n  [bt] (4) /home/hadih/.cache/pypoetry/virtualenvs/fids-rgvgQkLu-py3.11/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(XGBoosterBoostedRounds+0x31) [0x7fee8393c081]\n  [bt] (5) /usr/lib/libffi.so.8(+0x74f6) [0x7fef112e54f6]\n  [bt] (6) /usr/lib/libffi.so.8(+0x3f5e) [0x7fef112e1f5e]\n  [bt] (7) /usr/lib/libffi.so.8(ffi_call+0x123) [0x7fef112e4b73]\n  [bt] (8) /usr/lib/python3.11/lib-dynload/_ctypes.cpython-311-x86_64-linux-gnu.so(+0x8f6c) [0x7fef112f7f6c]\n\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "def f(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"The function to predict.\"\"\"\n",
        "    return x * np.sin(x)\n",
        "\n",
        "\n",
        "def quantile_loss(args: argparse.Namespace) -> None:\n",
        "    \"\"\"Train a quantile regression model.\"\"\"\n",
        "    rng = np.random.RandomState(1994)\n",
        "    # Generate a synthetic dataset for demo, the generate process is from the sklearn\n",
        "    # example.\n",
        "    X = np.atleast_2d(rng.uniform(0, 10.0, size=1000)).T\n",
        "    expected_y = f(X).ravel()\n",
        "\n",
        "    sigma = 0.5 + X.ravel() / 10.0\n",
        "    noise = rng.lognormal(sigma=sigma) - np.exp(sigma**2.0 / 2.0)\n",
        "    y = expected_y + noise\n",
        "\n",
        "    # Train on 0.05 and 0.95 quantiles. The model is similar to multi-class and\n",
        "    # multi-target models.\n",
        "    alpha = np.array([0.05, 0.5, 0.95])\n",
        "    evals_result: Dict[str, Dict] = {}\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rng)\n",
        "    # We will be using the `hist` tree method, quantile DMatrix can be used to preserve\n",
        "    # memory.\n",
        "    # Do not use the `exact` tree method for quantile regression, otherwise the\n",
        "    # performance might drop.\n",
        "    Xy = xgb.QuantileDMatrix(X, y)\n",
        "    # use Xy as a reference\n",
        "    Xy_test = xgb.QuantileDMatrix(X_test, y_test, ref=Xy)\n",
        "\n",
        "    booster = xgb.train(\n",
        "        {\n",
        "            # Use the quantile objective function.\n",
        "            \"objective\": \"reg:quantileerror\",\n",
        "            \"tree_method\": \"hist\",\n",
        "            \"quantile_alpha\": alpha,\n",
        "            # Let's try not to overfit.\n",
        "            \"learning_rate\": 0.04,\n",
        "            \"max_depth\": 5,\n",
        "        },\n",
        "        Xy,\n",
        "        num_boost_round=32,\n",
        "        early_stopping_rounds=2,\n",
        "        # The evaluation result is a weighted average across multiple quantiles.\n",
        "        evals=[(Xy, \"Train\"), (Xy_test, \"Test\")],\n",
        "        evals_result=evals_result,\n",
        "    )\n",
        "    xx = np.atleast_2d(np.linspace(0, 10, 1000)).T\n",
        "    scores = booster.inplace_predict(xx)\n",
        "    # dim 1 is the quantiles\n",
        "    assert scores.shape[0] == xx.shape[0]\n",
        "    assert scores.shape[1] == alpha.shape[0]\n",
        "\n",
        "    y_lower = scores[:, 0]  # alpha=0.05\n",
        "    y_med = scores[:, 1]  # alpha=0.5, median\n",
        "    y_upper = scores[:, 2]  # alpha=0.95\n",
        "\n",
        "    # Train a mse model for comparison\n",
        "    booster = xgb.train(\n",
        "        {\n",
        "            \"objective\": \"reg:squarederror\",\n",
        "            \"tree_method\": \"hist\",\n",
        "            # Let's try not to overfit.\n",
        "            \"learning_rate\": 0.04,\n",
        "            \"max_depth\": 5,\n",
        "        },\n",
        "        Xy,\n",
        "        num_boost_round=32,\n",
        "        early_stopping_rounds=2,\n",
        "        evals=[(Xy, \"Train\"), (Xy_test, \"Test\")],\n",
        "        evals_result=evals_result,\n",
        "    )\n",
        "    xx = np.atleast_2d(np.linspace(0, 10, 1000)).T\n",
        "    y_pred = booster.inplace_predict(xx)\n",
        "\n",
        "    if args.plot or True:\n",
        "        from matplotlib import pyplot as plt\n",
        "\n",
        "        fig = plt.figure(figsize=(10, 10))\n",
        "        plt.plot(xx, f(xx), \"g:\", linewidth=3, label=r\"$f(x) = x\\,\\sin(x)$\")\n",
        "        plt.plot(X_test, y_test, \"b.\", markersize=10, label=\"Test observations\")\n",
        "        plt.plot(xx, y_med, \"r-\", label=\"Predicted median\")\n",
        "        plt.plot(xx, y_pred, \"m-\", label=\"Predicted mean\")\n",
        "        plt.plot(xx, y_upper, \"k-\")\n",
        "        plt.plot(xx, y_lower, \"k-\")\n",
        "        plt.fill_between(\n",
        "            xx.ravel(), y_lower, y_upper, alpha=0.4, label=\"Predicted 90% interval\"\n",
        "        )\n",
        "        plt.xlabel(\"$x$\")\n",
        "        plt.ylabel(\"$f(x)$\")\n",
        "        plt.ylim(-10, 25)\n",
        "        plt.legend(loc=\"upper left\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # parser = argparse.ArgumentParser()\n",
        "    # parser.add_argument(\n",
        "    #     \"--plot\",\n",
        "    #     action=\"store_true\",\n",
        "    #     help=\"Specify it to enable plotting the outputs.\",\n",
        "    # )\n",
        "    # args = parser.parse_args()\n",
        "    quantile_loss(args=None)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
